version: '3.8'

services:
  crawler:
    build:
      context: .
      dockerfile: Dockerfile.crawler
    container_name: web-crawler-server
    ports:
      - "5001:5001"
    environment:
      - CRAWLER_PORT=5001
      - DEBUG=False
      - HTTP_PROXY=http://192.168.31.22:10808
      - HTTPS_PROXY=http://192.168.31.22:10808
      - http_proxy=http://192.168.31.22:10808
      - https_proxy=http://192.168.31.22:10808
      - NO_PROXY=localhost,127.0.0.1,172.18.0.0/16,10.0.0.0/8,192.168.0.0/16
      - no_proxy=localhost,127.0.0.1,172.18.0.0/16,10.0.0.0/8,192.168.0.0/16
      - CONTENT_DB_PATH=/app/data/web_crawler.db
      - URL_HISTORY_DB_PATH=/app/data/url_history.db
    volumes:
      - /mnt/rbd0/crawler_data:/app/data
    networks:
      - crawler-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:5001/api/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  ui:
    build:
      context: .
      dockerfile: Dockerfile.ui
    container_name: web-crawler-ui
    ports:
      - "5002:5002"
    environment:
      - UI_PORT=5002
      - CRAWLER_SERVER_URL=http://crawler:5001
      - DEBUG=False
    depends_on:
      crawler:
        condition: service_healthy
    networks:
      - crawler-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import requests; requests.get('http://localhost:5002/')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s


volumes:
  crawler_data:

networks:
  crawler-network:
    driver: bridge 